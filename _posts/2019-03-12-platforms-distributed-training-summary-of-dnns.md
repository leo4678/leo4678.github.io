---
layout: post
category: "platforms"
title: "深度神经网络分布式训练方法"
tags: [深度网络分布式训练，分布式随机梯度下降的同步和异步变体，各种All Reduce梯度聚合策略]
urlcolor: blue
---

目录

<!-- TOC -->

- [1. 分布式训练产生原因](#1-分布式训练产生原因)
- [2. 分布式训练是什么](#2-分布式训练是什么)
- [3. 分布式训练策略](#3-分布式训练策略)
	- [3.1 分布式训练算法（同步/异步SGD，Adam，RMSProp）](#31-分布式训练算法（同步/异步SGD，Adam，RMSProp）)
	- [3.2 节点之间的通信](#32-节点之间的通信)
	- [3.3 调整批量大小](#33-调整批量大小)
	- [3.4 混合精度训练](#34-混合精度训练)
	- [3.5 梯度和参数压缩](#35-梯度和参数压缩)
- [4. 总结](#4-总结)

<!-- /TOC -->

独立研究者 Karanbir Chahal 和 Manraj Singh Grover 与 IBM 的研究者 Kuntal Dey 近日发布了一篇论文，对深度神经网络的分布式训练方法进行了全面系统的总结，其中涉及到训练算法、优化技巧和节点之间的通信方法等。

[论文下载](https://leo4678.github.io/assets/AHitchhikersGuideOnDistributedTrainingOfDNNs.pdf)

[原始论文链接](https://arxiv.org/abs/1810.11787)

## 1. 分布式训练产生原因

自从深度学习提出以来，已经在视觉、语言、推荐等领域取得非常好的效果。但不幸的是，这种算法需要大量的数据才能有效的完成训练。因此，训练耗时非常高，如：第一个在ImageNet分类任务上取得当前最佳结果的深度学习算法在单个GPU上训练足足耗费了一周时间。这是难以接受的，有什么方法能够加速深度学习的训练呢？回答是分布式训练。

## 2. 分布式训练是什么

神经网络的分布式训练有两种方法来实现：数据并行和模型并行。

- 数据并行

数据并行化的目标是将数据集均等地分配到系统的各个训练节点，其中每个节点都有该神经网络的一个副本及其本地的权重。每个节点都会处理该数据集的一个不同子集并更新其本地权重集。这些本地权重会在整个集群中共享，从而通过一个累积算法计算出一个新的全局权重集。这些全局权重又会被分配至所有节点，然后节点会在此基础上处理下一批数据。

- 模型并行

模型并行化则是通过将该模型的架构切分到不同的节点上来实现训练的分布化。AlexNet是使用模型并行化的最早期模型之一，其方法是将网络分摊到 2个GPU上以便模型能放入内存中。**当模型架构过大以至于无法放入单台机器且该模型的某些部件可以并行化时，才能应用模型并行化。模型并行化可用在某些模型中，比如目标检测网络，这种模型的绘制边界框和类别预测部分是相互独立的。一般而言，大多数网络只可以分配到2个GPU上，这限制了可实现的可扩展数量，因此本论文主要关注的是数据并行化。**

## 3. 分布式训练策略

数据并行需要解决问题：

### 3.1 分布式训练算法（同步/异步SGD，Adam，RMSProp）

> 同步SGD：网络中的节点首先在它们的本地数据批上计算出梯度，然后每个节点都将它们的梯度发送给主服务器（master server）。主服务器通过求这些梯度的平均来累积这些梯度，从而为权重更新步骤构建出新的全局梯度集。这些全局梯度会通过使用与单机器 SGD 同样的配方来更新每个节点的本地权重。这整个过程都类似于在单台机器上通过单个数据 mini-batch 计算前向通过和反向传播步骤，因此同步 SGD 能保证收敛。但是同步 SGD 也存在一些局限性，如：每一个batch数据训练依赖的问题（也成同步屏障问题/synchronization barrier）。

> 异步SGD：每个模型副本都会向参数服务器请求全局权重，处理一个 mini-batch 来计算梯度并将它们发回参数服务器，然后参数服务器会据此更新全局权重。因为每个节点都独立计算梯度且无需彼此之间的交互，所以它们可以按自己的步调工作，也对机器故障更为稳健，即如果一个节点故障，其它节点还能继续处理，因此能消除由同步 SGD 引入的同步屏障（synchronization barrier）问题。

### 3.2 节点之间的通信

- Ring All Reduce算法：分两步，先scatter reduce，后all gather。scatter reduce环装合并，all gather同步数据到每一个节点。
- 递归减半或倍增：与上述算法不同点在，在scatter reduce之前，进行向量的合并，达到缩减Ring All Reduce机器数的目的。
- Binary Blocks：Binary Blocks 算法是对递归距离倍增和向量减半算法的延展，它的目标是当机器数量不是 2 的乘方数时降低负载的不平衡程度。举个例子，对于600台机器构成的集群，可以分成4组，其中每组各有2^9、2^6、2^4、2^3 台机器。
- 容错式All Reduce

### 3.3 调整批量大小

较大的 mini-batch 大小具有一些优势，主要的一个优势是：在大 mini-batch 上训练时，模型能以更大的步幅到达局部最小值，由此能够加快优化过程的速度。但是在实践中，使用大批量会导致网络的测试准确度有时会低于在更小批量上训练的模型。最近的一些研究通过与批量大小成比例地调整学习率而实现了在大批量上的训练。**实验发现，增加批量大小就相当于降低学习率，而使用大批量进行训练还有一个额外的好处，即在训练中所要更新的总参数更少**。

### 3.4 混合精度训练

神经网络最初是使用单精度或双精度数作为默认数据类型，因为这些数据类型在获取网络想要建模的任务的表征上表现很好。单精度数是32位浮数，双精度数是64位浮点数。近期有研究表明通过在更低精度数据类型进行训练，可将神经网络的速度和大小降低 50%-80%。更具体而言，将低精度梯度与学习率相乘有时会导致数值溢出16位的范围，从而导致计算不正确，进而导致最终验证准确度损失。混合精度训练的目标是通过使用单精度（32 位）的权重主副本并以半精度（16 位）运行其它一切来解决这个问题。混合精度训练能实现更高的吞吐量，从而能降低计算与通信瓶颈。但是，混合精度训练也存在一些需要注意的是损失丢失和算术精度更低。

### 3.5 梯度和参数压缩

简而言之，对更新梯度采用一定策略来进行压缩。

## 4. 总结

为了打造一个高效且可扩展的分布式训练框架，我们推荐使用以下技术：

- 推荐使用同步SGD算法来进行训练，因为它有严格的收敛保证。
- 应该使用 Binary Blocks 算法来执行梯度累积的 All Reduce 流程，因为其运行时间更优。
- 为了有效地使用硬件和网络带宽，应该在框架中使用梯度压缩、量化和混合精度训练等多种技术。我们推荐组合式地使用深度梯度压缩和混合精度练。
- 应该使用非常大的批量大小来进行训练，以最大化并行能力和最小化运行时间。我们推荐使用 LARS 算法，因为已有研究证明它能够以高达 64000的批量大小足够稳健地训练网络。
