---
layout: post
category: "ml"
title: "数据预处理之索引编码"
tags: [机器学习基础知识]
urlcolor: blue
---

目录

<!-- TOC -->

- [1. 什么是特征索引编码？](#1-什么是特征索引编码？)
- [2. 为什么需要一种高效易用的索引编码方法？](#2-为什么需要一种高效易用的索引编码方法？)
- [3. Hash索引编码方法](#3-Hash索引编码方法)
	- [3.1 实现细节](#31-实现细节)
	- [3.2 统计类索引编码方法与Hash索引编码方法对比](#32-统计类索引编码方法与Hash索引编码方法对比)
- [4. 高效及可用性验证](#4-高效及可用性验证)
	- [4.1 实验验证](#41-实验验证)
	- [4.2 建议](#42-建议)
- [5. 总结](#5-总结)

<!-- /TOC -->

> 导语： 在机器学习领域，准备模型训练使用的数据时，有一个必不可少的环节是将样本中的特征明文转为稀疏向量(如：{'age': 20，'gender': 2，...} -> {1: 20，3: 2，...})。本文所描述的“基于Hash的特征索引编码方法”能够高效的处理这一步，特别适合在线学习和海量样本使用。

## 1. 什么是特征索引编码？

在使用机器学习方法对一个问题进行建模时，首先要做的一项工作是收集可用数据，使用收集到的数据来描述一个客观存在的事实。在实际业务中，收集数据大多会汇总用户行为，用户画像，物品画像，及各类基于人类先验知识的统计类数据，最终呈现出来的数据格式如下:

```
sample 1 -> lable: 1.0; features: {点击行为->餐饮类次数: 23.0, 性别->男性: 1.0, 年龄->20-30岁之间： 1.0, 物品->类别为餐饮: 1.0, 物品100085->点击率: 0.02, ...}
sample 2 -> lable: 0.0; features: {点击行为->零售类次数: 12.0, 性别->女性: 1.0, 年龄->10-20岁之间： 1.0, 物品->类别为零售: 1.0, 物品100080->点击率: 0.015, ...}
...
```

上述数据每一行是一条样本， 如：sample 1的因变量为1.0，自变量为“点击餐饮类次数为23.0, 性别为男性, 年龄在20-30岁之间, 物品类别为餐饮, 物品100085点击率为0.02, ...”，这种类型的数据可以称之为明文数据。机器并不能对明文数据进行计算，它只能对数值进行计算，因此需要将明文数据转为向量，最终输入给机器的数据格式一般如下：

```
1.0 1:23.0 3:1.0 5:1.0 8:1.0 12:0.02 ...
0.0 2:12.0 4:1.0 6:1.0 9:1.0 11:0.015 ...
...
```

对于第一行，`1.0`为label(对应sample 1中的因变量)，`1:23.0 3:1.0 5:1.0 8:1.0 12:0.02 ...`为特征(对应sample 1中的自变量，分别为“点击行为->餐饮类次数”，“性别->男性”，“年龄->20-30岁之间”，“物品->类别为餐饮”，“物品100085->点击率”分配的向量索引为`1 3 5 8 12`)

> 定义：本文将原始明文数据转为向量的过程称为特征索引编码

## 2. 为什么需要一种高效易用的索引编码方法？

特征索引编码常用统计类方法，即先对特征中的key(如：样本示例中的`点击行为->餐饮类次数，性别->男性`等)进行统计，为每一个key分配全局唯一索引ID。该方法适合在少量样本且离线训练时使用，不适合在在线学习或海量样本训练时使用

- 为什么在线学习不适合？

在线学习的目的是为了更加快速的捕捉用户兴趣变化，使模型更加契合当时的数据分布，一般采用增量学习的方法来进行训练。由于是增量学习，所以不能预先对数据进行统计，除非是预先划定特征集范围，将流式训练时范围外的特征扔掉。这种处理方式不仅不优雅，同时也可能和业务目标不相符。举个例子，在电商领域，特别是大促期(如：618，双11等)商品的更新频率非常高，对于新上架的物品，会实时产生大量跟新品相关的特征，对于新特征，要么不用，要么为模型增加很多复杂的逻辑来适配这种新特征(如：预先估计特征种类，划定编码范围，做一大堆数据预处理逻辑)。那么有没有一种优雅的方式来处理在线学习增量更新时的特征索引编码呢？

注：增量学习指的是先离线训练一个base模型，然后基于实时产生的数据流，调整base模型使之更加契合实时数据分布

- 为什么海量样本数据不适合？

海量样本时，由于样本量非常大，对其进行全局特征统计的成本非常高，有可能“特征索引编码”所用的时间超过了模型训练所用时间，成为模型优化的一大瓶颈。那么有没有一种高效的“特征索引编码”方法呢？

基于上述两个问题，算法工程师需要一个使用方便、高效的特征索引编码工具

## 3. Hash索引编码方法

“特征索引编码”本质上就是使用一种映射关系，对明文数据进行转换

“统计类特征索引编码”先统计样本集中的特征集，然后生成特征Key到ID的映射关系。当样本量很大时，统计的成本很高；当样本不固定时，统计的结果有可能不能全量覆盖所有数据。这个时候使用“统计类特征索引编码”方法不太合适

考虑到Hash函数本质上也是一种映射关系，能够将全量扫描变为一次计算，时间复杂度由O(n)变为O(1)。对于“特征索引编码”任务，可以尝试使用一种分布均匀、低冲突率的Hash函数来生成这个映射关系

### 3.1 实现细节

<html>
<br/>
<img src='/assets/hash_index_proce.png' style='max-height:336px;max-width:362px;'/>
<br/>
</html>

#### 特征表达结构

一般情况下，特征分离散型和连续型，离散型一般表达为【维度，等级，特征值】，如【性别，男性，1.0】，【类别，零售，1.0】等；连续型一般表达为【维度，特征值】，如【点击率，0.334】，【曝光次数，1234】等。为了适配离散型和连续型两种特征表达结构，这里使用【主键，辅键，特征值】结构来表达特征，对于离散型特征，`主键对应维度，辅键对应等级，特征值对应特征值`，对于连续型特征，`主键对应维度，辅键全局统一占位符（无实际意义），特征值对应特征值`。

#### Hash索引编码器

Hash索引编码器的作用是将字符串映射到某一区间范围的整数值，同时尽可能保证低的冲突率。其操作过程如下：

<html>
<br/>
<img src='/assets/hash_function.png' style='max-height:770px;max-width:306px;'/>
<br/>
</html>

上述流程图简单说明了，Hash函数如何将一个字符串转为一个索引。现有的Hash函数有很多，包括`Checksum (8, 16, 32, or 64 bit)，CRC16 (16 bit)，CRC32 (32 bit)，MD5 (128 bit)，SHA-1 (160 bit)，SHA-256 (256 bit)，RipeMD-128 (128 bit)，RipeMD-160 (160 bit)，MD4 (128 bit)，Ed2k (128 bit)，Adler32，MurmurHash3 (32, 128 bit)`。考虑到[MurmurHash3](https://zh.wikipedia.org/wiki/Murmur%E5%93%88%E5%B8%8C)对于规律性较强的key(特征key一般为各种英文单词及数字的组合)，随机分布特征表现更加良好，所以Hash索引编码使用Murmur3算法。目前已实现MurmurHash算法的语言包括C++、Python、C、C#、Perl、Ruby、PHP、Scala、Java、JavaScript，Spark中也实现了32bit的[Murmur3_x86_32](https://github.com/apache/spark/blob/master/common/unsafe/src/main/java/org/apache/spark/unsafe/hash/Murmur3_x86_32.java)，对于使用Spark处理大数据的应用程序可以直接调用该API来完成Hash索引编码的功能。

#### 示例代码

```scala
import org.apache.spark.mllib.feature.HashingTF
import scala.math 

/**
 * 特征key映射，策略为将最终映射区间分为两部分，一部分为主键映射区，一部分为辅键映射区。这样设计的目的是尽可能使用主键、辅键不同则映射索引肯定不同的先验知识来保证低冲突率
 * 
 * @param pk 特征主键
 * @param sk 特征辅键
 * @param pkMax 模型所用特征集合中，包含的主键个数
 * @param skMax 模型所用特征集合中，包含的主键所属辅键最大个数
 * @param pkScaling 使用hash算法映射时，主键映射区间扩展倍数，映射区间越大，越不容易冲突
 * @param skScaling 使用hash算法映射时，辅键映射区间扩展倍数，映射区间越大，越不容易冲突
 * @return 映射索引
*/
def hashIndex(pk: String, sk: String, pkMax: Int, skMax: Int, pkScaling: Int, skScaling: Int): Int = {
	val pkTF = new HashingTF(pkMax*pkScaling)
	val skTF = new HashingTF(skMax*skScaling)
	val pkLength = math.log10(pkMax*pkScaling.toDouble).ceil.toInt //pk最长用多少位表示
	val pkIx = s"%0${pkLength}d".format(pkTF.indexOf(pk))
	val skIx = skTF.indexOf(sk)
	s"$skIx$pkIx".toInt
}
```

#### 冲突实验测试

<html>
<br/>
<img src='/assets/hash_conflit_lab.png' style='max-height:150px;max-width:430px;'/>
<br/>
</html>

上表指的是特征维度(汇总主键和辅键)分别在千维、万维、十万维到亿维时，映射区间为2^32-1时，冲突比率大小。

**结论：上表测试数据，使用实际业务中特征数据测试。一般的，主键的映射区间是主键维度数的5-10倍，辅键的映射区间是最大辅键个数的2-5倍，就能保证较低的冲突率。**

### 3.2 统计类索引编码方法与Hash索引编码方法对比

<html>
<br/>
<img src='/assets/feature_ix_compare.png' style='max-height:264px;max-width:1162px;'/>
<br/>
</html>

## 4. 高效及可用性验证

### 4.1 实验验证

为了验证该方法的可靠性，使用业务真实数据，对Hash特征索引编码方式下的不同模型进行了评估指标的对比。从模型训练的评估指标上看，该方法是可靠的

<html>
<br/>
<img src='/assets/hash_index_model_lab.png' style='max-height:268px;max-width:1060px;'/>
<br/>
</html>

### 4.2 建议

笔者使用Spark对数据进行处理，这里给出Spark环境下建议。由于Spark原生Murmur3_x86_32对索引有最大值限制INT.MAX_VALUE(2^32-1)，经过验证，千万维特征规模使用原生Murmur3_x86_32是可靠的。更高维度特征的需求，需要复写该算法，扩大特征映射区间

## 5. 总结

本文描述了模型训练数据预处理阶段特征索引编码的一种易用、高效的方法，包括该方法的原理、实现、实验以及建议等，希望能对读者有所帮助

参考：

1. [论文: Feature Hashing for Large Scale Multitask Learning](https://arxiv.org/abs/0902.2206)
2. [Murmur哈希](https://zh.wikipedia.org/wiki/Murmur%E5%93%88%E5%B8%8C)